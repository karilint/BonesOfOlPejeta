{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c956a8d",
   "metadata": {},
   "source": [
    "## Rarefaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88feeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import comb\n",
    "\n",
    "RNG = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transect and occurrence data\n",
    "# Paths assume notebook resides in notebooks/ directory\n",
    "transects_path = '../data/pkl/df_transects.pkl'\n",
    "occurrences_path = '../data/pkl/df_occurrences_with_taxon.pkl'\n",
    "\n",
    "try:\n",
    "    df_transects = pd.read_pickle(transects_path)\n",
    "    df_occurrences = pd.read_pickle(occurrences_path)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"Required data file missing: {e}\")\n",
    "\n",
    "# Filter transects: exclude 2008 and 2024 except 'shrubs closed'; only old reserve\n",
    "\n",
    "df_transects['Year'] = pd.to_datetime(df_transects['start_time']).dt.year\n",
    "mask_not_2008 = df_transects['Year'] != 2008\n",
    "mask_keep_2024 = ~((df_transects['Year'] == 2024) & (df_transects['Pre: Transect physical habitat'] != 'shrubs closed'))\n",
    "df_filtered_transects = df_transects[mask_not_2008 & mask_keep_2024].copy()\n",
    "df_filtered_transects = df_filtered_transects[df_filtered_transects['Pre: On old reserve?'] == 'Yes'].copy()\n",
    "\n",
    "rename_transects = {'UID': 'TransectID', 'Pre: Transect physical habitat': 'Habitat'}\n",
    "df_filtered_transects = df_filtered_transects.rename(columns=rename_transects)\n",
    "\n",
    "# Prepare occurrences\n",
    "rename_occ = {'UID': 'TransectID', 'Taxon Label': 'Taxon'}\n",
    "df_occurrences = df_occurrences.rename(columns=rename_occ)\n",
    "df_occurrences = df_occurrences[df_occurrences['TransectID'].isin(df_filtered_transects['TransectID'])]\n",
    "\n",
    "excluded_species = ['ostrich', 'Aves (medium)', 'Aves (small)']\n",
    "df_occurrences = df_occurrences[~df_occurrences['Taxon'].isin(excluded_species)]\n",
    "\n",
    "df_occurrences = df_occurrences.merge(\n",
    "    df_filtered_transects[['TransectID', 'Habitat']],\n",
    "    on='TransectID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "habitats = sorted(df_occurrences['Habitat'].dropna().unique())\n",
    "print('Habitats:', habitats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55164dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_incidence_matrix(df, habitat):\n",
    "    subset = df[df['Habitat'] == habitat]\n",
    "    matrix = (subset\n",
    "              .pivot_table(index='TransectID', columns='Taxon', aggfunc='size', fill_value=0)\n",
    "              .astype(int))\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def sample_coverage(counts):\n",
    "    t = counts.sum()\n",
    "    if t == 0:\n",
    "        return 0.0\n",
    "    freq = (counts > 0).sum(axis=0)\n",
    "    Q1 = (freq == 1).sum()\n",
    "    Q2 = (freq == 2).sum()\n",
    "    if t == 1:\n",
    "        return 1 - Q1 / t\n",
    "    return 1 - (Q1 / t) * ((t - 1) * Q1 / ((t - 1) * Q1 + 2 * Q2))\n",
    "\n",
    "\n",
    "def accumulate(matrix, max_samples=None, n_iter=200, extrapolate=0):\n",
    "    n_transects = matrix.shape[0]\n",
    "    species = matrix.shape[1]\n",
    "    if max_samples is None:\n",
    "        max_samples = n_transects + extrapolate\n",
    "    richness = np.zeros((n_iter, max_samples), dtype=float)\n",
    "    coverage = np.zeros((n_iter, max_samples), dtype=float)\n",
    "    ids = matrix.index.to_numpy()\n",
    "    data = matrix.to_numpy()\n",
    "    for i in range(n_iter):\n",
    "        order = RNG.permutation(n_transects)\n",
    "        for k in range(max_samples):\n",
    "            if k < n_transects:\n",
    "                sample = data[order[:k+1]]\n",
    "            else:\n",
    "                extra = RNG.choice(n_transects, size=k+1-n_transects, replace=True)\n",
    "                sample = np.vstack([data[order], data[extra]])\n",
    "            counts = (sample.sum(axis=0) > 0).astype(int)\n",
    "            richness[i, k] = counts.sum()\n",
    "            coverage[i, k] = sample_coverage(sample)\n",
    "    mean_r = richness.mean(axis=0)\n",
    "    ci_r = np.percentile(richness, [2.5, 97.5], axis=0)\n",
    "    mean_c = coverage.mean(axis=0)\n",
    "    ci_c = np.percentile(coverage, [2.5, 97.5], axis=0)\n",
    "    return mean_r, ci_r, mean_c, ci_c\n",
    "\n",
    "\n",
    "def chao2_incidence(mat):\n",
    "    inc = (mat > 0).sum(axis=0)\n",
    "    Sobs = (inc > 0).sum()\n",
    "    Q1 = (inc == 1).sum()\n",
    "    Q2 = (inc == 2).sum()\n",
    "    return Sobs + (Q1 * Q1) / (2 * Q2) if Q2 > 0 else Sobs + (Q1 * (Q1 - 1)) / (2 * (Q2 + 1))\n",
    "\n",
    "\n",
    "def chao2_bootstrap(mat, n_boot=200):\n",
    "    est = chao2_incidence(mat)\n",
    "    boots = np.empty(n_boot)\n",
    "    for i in range(n_boot):\n",
    "        sample = mat.sample(n=mat.shape[0], replace=True)\n",
    "        boots[i] = chao2_incidence(sample)\n",
    "    lo_p, hi_p = np.percentile(boots, [2.5, 97.5])\n",
    "    ci_percentile = (lo_p, hi_p)\n",
    "    ci_basic = (2 * est - hi_p, 2 * est - lo_p)\n",
    "    bias = boots.mean() - est\n",
    "    return est, np.array(ci_percentile), np.array(ci_basic), bias, boots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "all_curves = {}\n",
    "boots_by_hab = {}\n",
    "for hab in habitats:\n",
    "    mat = build_incidence_matrix(df_occurrences, hab)\n",
    "    mean_r, ci_r, mean_c, ci_c = accumulate(mat, max_samples=mat.shape[0]*2)\n",
    "    est, ci_p, ci_b, bias, boots = chao2_bootstrap(mat)\n",
    "    boots_by_hab[hab] = boots\n",
    "    all_curves[hab] = {\n",
    "        'mean_r': mean_r,\n",
    "        'ci_r': ci_r,\n",
    "        'mean_c': mean_c,\n",
    "        'ci_c': ci_c,\n",
    "        'chao2': est,\n",
    "        'chao_ci_percentile': ci_p,\n",
    "        'chao_ci_basic': ci_b,\n",
    "        'bias': bias,\n",
    "        'n_transects': mat.shape[0]\n",
    "    }\n",
    "    results.append({\n",
    "        'Habitat': hab,\n",
    "        'Chao2': est,\n",
    "        'Bias': bias,\n",
    "        'Chao2_lower_basic': ci_b[0],\n",
    "        'Chao2_upper_basic': ci_b[1],\n",
    "        'Chao2_lower_percentile': ci_p[0],\n",
    "        'Chao2_upper_percentile': ci_p[1]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC: recompute Chao2 directly from matrices\n",
    "qc_records = []\n",
    "for hab in habitats:\n",
    "    mat = build_incidence_matrix(df_occurrences, hab)\n",
    "    direct = chao2_incidence(mat)\n",
    "    table_val = summary_df.loc[summary_df['Habitat']==hab, 'Chao2'].iloc[0]\n",
    "    qc_records.append({'Habitat': hab, 'DirectChao2': direct, 'TableChao2': table_val})\n",
    "pd.DataFrame(qc_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample-based accumulation plots\n",
    "fig, axes = plt.subplots(len(habitats), 2, figsize=(12, 4 * len(habitats)))\n",
    "for i, hab in enumerate(habitats):\n",
    "    curves = all_curves[hab]\n",
    "    # Accumulation\n",
    "    ax = axes[i,0] if len(habitats) > 1 else axes[0]\n",
    "    x = np.arange(1, len(curves['mean_r']) + 1)\n",
    "    ax.plot(x, curves['mean_r'], label='Mean richness')\n",
    "    ax.fill_between(x, curves['ci_r'][0], curves['ci_r'][1], alpha=0.3)\n",
    "    ax.set_title(f'{hab} - Sample-based')\n",
    "    ax.set_xlabel('Number of transects')\n",
    "    ax.set_ylabel('Species richness')\n",
    "    # Coverage-based\n",
    "    ax2 = axes[i,1] if len(habitats) > 1 else axes[1]\n",
    "    ax2.plot(curves['mean_c'], curves['mean_r'])\n",
    "    ax2.fill_between(curves['mean_c'], curves['ci_r'][0], curves['ci_r'][1], alpha=0.3)\n",
    "    ax2.set_title(f'{hab} - Coverage-based')\n",
    "    ax2.set_xlabel('Sample coverage')\n",
    "    ax2.set_ylabel('Species richness')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Richness at equal effort (10 transects) and equal coverage (95%)\n",
    "records = []\n",
    "for hab in habitats:\n",
    "    curves = all_curves[hab]\n",
    "    x = np.arange(1, len(curves['mean_r']) + 1)\n",
    "    # Equal effort\n",
    "    k = min(10, curves['n_transects'])\n",
    "    richness_k = curves['mean_r'][k-1]\n",
    "    # Equal coverage\n",
    "    target_cov = 0.95\n",
    "    cov = curves['mean_c']\n",
    "    if (cov >= target_cov).any():\n",
    "        idx = np.argmax(cov >= target_cov)\n",
    "        richness_cov = curves['mean_r'][idx]\n",
    "    else:\n",
    "        richness_cov = np.nan\n",
    "    records.append({'Habitat': hab, 'Richness@10': richness_k, 'Richness@95%cov': richness_cov})\n",
    "richness_df = pd.DataFrame(records)\n",
    "richness_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc76ecf",
   "metadata": {},
   "source": [
    "Curves are based on incidence (presence/absence). Detectability likely varies among habitats, so coverage-based comparisons complement equal-effort curves."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}